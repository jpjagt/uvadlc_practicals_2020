# -*- org-export-babel-evaluate: nil -*-
#+BIND: org-export-use-babel nil
#+TITLE: DLC assignment 1: MLPs, CNNs & backpropagation
#+AUTHOR: Jeroen Jagt@@latex:\\@@Master Artifical Intelligence@@latex:\\@@Universiteit van Amsterdam@@latex:\\@@jpjagt@pm.me
#+DATE: October 29, 2020
# #+STARTUP:
#+LATEX: \setlength\parindent{0pt}
#+LaTeX_HEADER: \usepackage[final]{nips_2018}
#+LaTeX_HEADER: \usepackage[utf8]{inputenc} % allow utf-8 input
#+LaTeX_HEADER: \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
#+LaTeX_HEADER: \usepackage{hyperref}       % hyperlinks
#+LaTeX_HEADER: \usepackage{url}            % simple URL typesetting
#+LaTeX_HEADER: \usepackage{booktabs}       % professional-quality tables
#+LaTeX_HEADER: \usepackage{amsfonts}       % blackboard math symbols
#+LaTeX_HEADER: \usepackage{nicefrac}       % compact symbols for 1/2, etc.
#+LaTeX_HEADER: \usepackage{microtype}      % microtypography
#+PROPERTY: header-args :exports both :session report :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex



#+BEGIN_EXPORT latex
\newcommand{\bt}[1]{\mathbf{#1}}
\newcommand{\dr}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\ykl}{\sum_m^M (X_{km} W_{lm}) + b_l}
\newcommand{\drs}[1]{\sum_{k,l}^{S,N} \dr{L}{Y_{kl}} \dr{Y_{kl}}{#1}}
\newcommand{\drsm}[1]{\sum_{k,l}^{S,M} \dr{L}{Y_{kl}} \dr{Y_{kl}}{#1}}
#+END_EXPORT

* MLP backprop

** gradients

*** a

We can write

$$ Y_{kl} = \sum_m^M (X_{km} W_{lm}) + B_{kl} = \sum_m^M (X_{km} W_{lm}) + b_l $$

#+BEGIN_EXPORT latex
Derivatives towards $\bf{W}$:

\begin{equation}
\dr{Y_{kl}}{W_{ij}} = \dr{}{W_{ij}} (\ykl) = \begin{cases} X_{kj} &\text{if
}i=l \\ 0 &\text{otherwise} \end{cases}
\end{equation}

Then,

$$\dr{L}{W_{ij}} = \drs{W_{ij}} = \sum_k^S \dr{L}{Y_{ki}} X_{kj} = (\dr{L}{\bt Y_{:i}})^T \bt X_{:j}$$ (where the index $:$ denotes all elements of that dimension).

Looking at the dimensions, we then see that

$$\dr{L}{{\bf W}} = (\dr{L}{\bt Y})^T \bt X$$

Derivatives towards $\bf{b}$:

$$\dr{Y_{kl}}{b_i} = \dr{}{b_i} (\ykl) = \begin{cases} 1 &\text{if }i = l \\ 0 &\text{otherwise} \end{cases}$$

Then,

$$\dr{L}{b_i} = \drs{b_i} = \sum_k^S \dr{L}{Y_{ki}} 1 = (\dr{L}{\bt Y_{:i}})^T \bt 1_S$$

where $\bt 1_S$ denotes a vector of ones of length $S$.

Finally:

$$\dr{L}{\bt b} = (\dr{L}{\bt Y})^T \bt 1_S$$

Derivatives towards $\bt X$:

$$\dr{Y_{kl}}{X_{ij}} = \dr{}{X_{ij}} (\ykl) = \begin{cases} W_{lj} &\text{if
  }k=i \\ 0 &\text{otherwise} \end{cases}$$

Then,

$$\dr{L}{X_{ij}} = \drs{X_{ij}} = \sum_l^N (\dr{L}{Y_{il}} W_{lj}) = \dr{L}{\bt Y_{i:}} \bt W_{:j}$$

And thus,

$$\dr{L}{\bt X} = \dr{L}{\bt Y} \bt W$$
#+END_EXPORT

*** b

#+BEGIN_EXPORT latex
For any such function $h$:

$$\dr{Y_{kl}}{X_{ij}} = \dr{}{X_{ij}} h(X_{kl}) =
\begin{cases}\dr{h}{X_{ij}} &\text{if }i=k \land j=l \\ 0 &\text{otherwise}\end{cases}$$

$$\dr{L}{X_{ij}} = \drs{X_{ij}} = \dr{L}{Y_{ij}} \dr{h}{X_{ij}}$$

The derivative of the ELU is:

$$\dr{h}{x} = \begin{cases} 1 &\text{if }x \geq 0 \\ e^x &\text{otherwise} \end{cases}$$
$$= \exp (x \times \mathbbm{1}[x < 0])$$

For the entire data batch, we can thus write:

$$\dr{L}{X_{ij}} = \drs{X_{ij}} = \dr{L}{Y_{ij}} \circ \bt{\text{exp}} (\bt X \circ \bt \mathbbm{1}[\bt X < 0])$$

where $\bt{\text{exp}}$ is the exponential function applied element-wise, and $\bt \mathbbm{1}[\cdot]$ is the indicator function applied element-wise on a (boolean) matrix.
#+END_EXPORT

*** c

#+BEGIN_EXPORT latex
I. Softmax derivative

\newcommand{\sm}{[\text{softmax}(\bt X)]}
\newcommand{\smsum}{\sum^C_c \exp(X_{kc})}

$$Y_{kl} = \sm_{kl} = \frac{\exp(X_{kl})}{\sum^C_c \exp(X_{kc})}$$

$$\dr{Y_{kl}}{X_{ij}} = \dr{}{X_{ij}} \sm_{kl}$$

This derivative is:

\begin{aligned}[]
[ \text{if } i \neq k ] &= 0 \\
[\text{else if }l \neq j] &= \exp(X_{il}) \frac{1}{(\smsum)^2} (- \exp(X_{ij}) = \sm_{il} (- \sm_{ij}) \\
[\text{else }] &= \frac{\exp(X_{ij}) \smsum - \exp(X_{ij})\exp(X_{ij})}{(\smsum)^2} \\
&= \frac{\exp(X_{ij})}{\smsum} (\frac{\smsum}{\smsum} - \frac{\exp(X_{ij})}{\smsum}) = \sm_{ij} (1 - \sm_{ij})
\end{aligned}

which can be written as:

$$\begin{cases} 0 &\text{if } i \neq k \\ \sm_{il} (\delta_{lj} - \sm_{ij}) &\text{otherwise} \end{cases}$$

where $\delta_{lj}$ is the Kronecker delta, which equals 1 when $l = j$, and 0 otherwise.

Plugging that in, we end up with:

$$\dr{L}{X_{ij}} = \sum_{k,l} \dr{L}{Y_{kl}} \dr{Y_{kl}}{X_{ij}} = \sum_{l} \dr{L}{Y_{il}} \dr{Y_{il}}{X_{ij}} = \sum_{l} \dr{L}{Y_{il}} \sm_{il} (\delta_{lj} - \sm_{ij})$$

II. Loss derivative

$$\dr{L}{X_{ij}} = - \frac{1}{S} \dr{}{X_{ij}} \sum_{k,c}^{S,C} T_{kc} \log(X_{kc}) = - \frac{1}{S} T_{ij} \frac{1}{X_{ij}} 1 = - \frac{1}{S} \frac{T_{ij}}{X_{ij}}$$

So

$$\dr{L}{\bt X} = - \frac{1}{S} \bt T \circ \bt X^{-1}$$

#+END_EXPORT

** numpy MLP

The loss and accuracy scores for the training and test sets during training
using the default parameters provided in the training script
=train_mlp_numpy.py= are displayed in Figure [[fig:numpy_results]].

#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+CAPTION: Loss and accuracy during training of the numpy MLP. The accuracy is graphed using a dashed line.
#+label: fig:numpy_results
[[file:code/train_mlp_numpy_results.png]]

* PyTorch MLP

** Implementation

For the initial implementation, I used the =Adam= optimizer that is provided by PyTorch.

After writing the initial implementation, I have implemented the following
adaptations to the PyTorch MLP, with the goal of attaining increased test
accuracy.
- *Change of optimizer*: realizing that my network had quite a lot of
  parameters, I switched from =Adam= to =SGD=, because to my knowledge, =Adam=
  works best on small networks. I added a flag for the momentum parameter which
  I set to =0.9= by default, which is the value I kept using. This switch
  caused my network to jump from around 25% to 35% test accuracy.
- *Increase in batch size*: from what I heard, it is generally desirable to
  have a large batch size because this will stabilize the descent, and that the
  batch size parameter is mainly determined by computational resources. I
  increased the batch size from 200 to 600, but this did not seem to yield a
  significant improvement in test accuracy. Nevertheless, I continued using a
  batch size of 600.
- *Increase in layers*: in order to investigate whether a deeper network would
  yield improved performance, I increased the =dnn_hidden_units= from just a
  single layer of width 100 to the following sequence: =100, 30, 20, 20, 20=. I
  chose for a slim but deep architecture because I've learnt that depth is more
  important than width. I also increased the =max_steps= from 1400 to 6000 to
  accomodate for the larger architecture (which I guessed required more
  iterations to train). All in all, this did lead to an increased test accuracy, which jumped
  from some 35% to around 41%.
- *LR Scheduler*: my test accuracy was fluctuating quite erratically at the later
  stages of training, so I added a learning rate scheduler in order to refine
  the training at later stages.
- *Adding BatchNorm layers*: I've seen BatchNorm layers in other DL
  architectures, and it's said that they generally cause the model to converge
  quicker. I chose to add them after every non-final linear layer. Doing so
  caused a substantial jump in performance, namely around 6% increase in test
  accuracy.

After these changes, I still did not manage to achieve the desired test
accuracy of =0.52=. I played around with the network, and finally managed to
achieve a test accuracy of =52.02%= using the following params and model
architecture:

#+BEGIN_EXPORT latex
\begin{verbatim}
dnn_hidden_units : 100,50,20
learning_rate : 0.029
max_steps : 5000
batch_size : 1050
optimizer : SGD
optimizer_momentum : 0.9
activation_fn : ELU
scheduler : StepLR
scheduler_gamma : 0.61
scheduler_step_size : 900

model:
MLP(
  (nn): Sequential(
    (0): Linear(in_features=3072, out_features=100, bias=True)
    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ELU(alpha=1.0)
    (3): Linear(in_features=100, out_features=50, bias=True)
    (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=50, out_features=20, bias=True)
    (7): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ELU(alpha=1.0)
    (9): Linear(in_features=20, out_features=10, bias=True)
    (10): Softmax(dim=1)
  )
)
\end{verbatim}
#+END_EXPORT

I reached these numbers with the intuitions that:
- A larger batch size generally seemed to improve training
- Activation functions: ReLU and ELU worked equally well. Other PyTorch
  functions, such as Hardshrink, yielded a much worse performance.
- Using a LR scheduler helped to finetune performance at later stages of
  training. Perhaps it would be better to use the more dynamic
  =ReduceLROnPlateau=, since I tuned =StepLR= to decrease more or less when the
  network seemed to flatten out, but I have not experimented with it.
- Varying the configuration of the hidden units of the MLP yielded very erratic
  results, and there did not seem to be a particular structure that performed
  better than others. Therefore, I kept the architecture relatively simple,
  adding two additional hidden layers on top of the hidden layer in the default
  parameters.
- Letting the network train for a higher number of steps did not lead to an
  increase in performance; most of the times, the optimum was reached
  relatively early in the training process.

Finally, I played around with the learning rate and scheduler parameters rather
randomly until this result was achieved (test accuracy of =52.02%= in epoch
38). The loss and accuracy of this run is displayed in Figure [[fig:pytorch_mlp_results]].

# 1604477470
#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+CAPTION: Loss and accuracy during training of the PyTorch MLP. The accuracy is graphed using a dashed line.
#+label: fig:pytorch_mlp_results
[[file:code/1604477470_mlp_pytorch_results.png]]
# id: 6810249

** Tanh vs ELU

One defining characteristic of the Tanh function is that its range is $(-1, 1)$,
whereas the range of the ELU function is $(-1, \inf)$. This results in the fact
that all output values of the hidden layers would be clamped to $(-1,
1)$. Since these activation layers would only be used as hidden layers (rather
than their output being the model's output), I don't see how this would be
problematic; however, I might be overlooking some detail. <<TODO>>

The output of the Tanh function also means that its derivatives become very
small beyond the domain of $[-2, 2]$, while the ELU function has stronger
derivatives (~1) on the larger domain of $[-1, \inf]$. This means that models
using ELU activation functions tend to converge more quickly than when using
Tanh activation functions, which is a great benefit of ELU functions.

* My very own Normalization Layer

** automatic differentiation

(this section is just here to keep the correct section counts).

** Manual implementation

*** a

#+BEGIN_EXPORT latex
First, let's derive towards $\bt \gamma$:

$$\dr{Y_{kl}}{\gamma_i} = \delta_{li} \hat{X}_{kl}$$

Thus,

$$\dr{L}{\gamma_i} = \drsm{\gamma_i} = \sum_{k,l}^{S,M} \dr{L}{Y_{kl}} \delta_{li} \hat{X}_{kl} = \sum_k^S \dr{L}{Y_{ki}} \hat{X}_{ki} = (\dr{L}{\bt Y_{:i}})^T \hat{\bt X}_{:i}$$

$$\dr{L}{\bt \gamma} = \diag((\dr{L}{\bt Y})^T \hat{\bt X}) \text{ or } = (\dr{L}{\bt Y} \circ \hat{\bt X})^T \bt 1_S$$

The derivative towards $\bt \beta$:

$$\dr{Y_{kl}}{\beta_i} = \delta_{li}$$

$$\dr{L}{\beta_i} = \drsm{\beta_i} = \sum_{k,l}^{S,M} \dr{L}{Y_{kl}} \delta_{li} = \sum_k^S \dr{L}{Y_{ki}} = (\dr{L}{\bt Y_{:i}})^T \bt 1_S$$

$$\dr{L}{\bt \beta} = (\dr{L}{\bt Y})^T \bt 1_S$$

Finally, the derivative towards $\bt X$. Since (elements of) $\bt X$ are also present in $\sigma^2_k$ and $\mu_k$, let's first do their derivatives:

$$\dr{\mu_k}{X_{ri}} = \delta_{kr} \frac{1}{M}$$

\newcommand{\del}[1]{\delta_{#1}}
\newcommand{\fM}{\frac{1}{M}}
\newcommand{\sM}{\sum_{m}^{M}}

\begin{aligned}
\dr{\sigma_k^2}{X_{ri}} &= \delta_{kr} \frac{1}{M} \sum^M_m \dr{}{X_{ri}} (X_{rm} - \mu_r)^2\\
&= \del{kr} \fM \sM 2(X_{rm} - \mu_r)(\del{mi} - \del{rr} \fM)\\
&= \del{kr} \fM 2 \sM (X_{rm} \del{mi} - X_{rm} \fM - \mu_r \del{mi} \mu_r \fM)\\
&= \del{kr} \fM 2 (X_{ri} - \mu_r - \fM \sM(X_{rm} - \mu_r))\\
&= \del{kr} \fM 2 (X_{ri} - \mu_r - (\fM \sM X_{rm} - \fM \sM \mu_r))\\
&= \del{kr} \fM 2 (X_{ri} - \mu_r - (\mu_r - \mu_r))\\
&= \del{kr} \fM 2 (X_{ri} - \mu_r)\\
\end{aligned}

Then, we can write down:

\newcommand{\ve}{\left(\sigma^2_k + \epsilon\right)}

\begin{aligned}
\dr{\hat{X}_{kl}}{X_{ri}} &= \dr{}{X_{ri}} \frac{X_{kl} - \mu_k}{\sqrt{\sigma_k^2 + \epsilon}}\\
&= \del{kr} \frac{(\del{kr} \del{li} - \del{kr} \fM) \sqrt{ve} - \frac{1}{2} (\ve)^{-1/2} \dr{\sigma^2_k}{X_{ri}} (X_{kl} - \mu_k)}{\ve}\\
&= \del{kr} \frac{(\del{li} - \fM) \sqrt{ve} - \frac{1}{2} (\ve)^{-1/2} \frac{2}{M} (X_{ri} - \mu_r) (X_{kl} - \mu_k)}{\ve}\\
&= \del{kr} \frac{(\del{li} - \fM) \sqrt{ve} - (\ve)^{-1/2} \frac{1}{M} (X_{ri} - \mu_r) (X_{kl} - \mu_k)}{\ve}\\
\end{aligned}

Then,

\newcommand{\ver}{\left(\sigma^2 + \epsilon\right)}
\newcommand{\veb}{\left(\boldsymbol{\sigma}^2 + \epsilon\right)}

\begin{aligned}
\dr{L}{X_{ri}} &= \sum_{k, l}^{S, M} \dr{L}{Y_{kl}} \dr{Y_{kl}}{X_{ri}}\\
&= \sum_{k, l}^{S, M} \dr{L}{Y_{kl}} \gamma_l \dr{\hat{X}_{kl}}{X_{ri}}\\
&= \sum_{k, l}^{S, M} \dr{L}{Y_{kl}} \gamma_l \del{kr} \frac{(\del{li} - \fM) \sqrt{ve} - (\ve)^{-1/2} \frac{1}{M} (X_{ri} - \mu_r) (X_{kl} - \mu_k)}{\ve}\\
&= \sum_{l}^{M} \dr{L}{Y_{rl}} \gamma_l \frac{(\del{li} - \fM) \sqrt{\ver} - (\ver)^{-1/2} \frac{1}{M} (X_{ri} - \mu_r) (X_{rl} - \mu_r)}{\ver}\\
&= \sum_{l}^{M} \dr{L}{Y_{rl}} \gamma_l \frac{(\del{li} - \fM) \sqrt{\ver} - (\ver)^{-1/2} \frac{1}{M} (X_{ri} - \mu_r) (X_{rl} - \mu_r)}{\ver}\\
&= \sum_{l}^{M} \dr{L}{Y_{rl}} \gamma_l \left( \frac{\del{li}\sqrt{\ver}}{\ver} - \frac{\fM \sqrt{\ver}}{\ver} - \frac{(\ver)^{-1/2} \frac{1}{M} (X_{ri} - \mu_r) (X_{rl} - \mu_r)}{\ver} \right)\\
&= \dr{L}{Y_{il}} \gamma_i \frac{\sqrt{\ver}}{\ver} + \fM \sum_{l}^{M} \dr{L}{Y_{rl}} \gamma_l \left( - \frac{\sqrt{\ver}}{\ver} - \frac{(\ver)^{-1/2} (X_{ri} - \mu_r) (X_{rl} - \mu_r)}{\ver} \right)\\
&= \dr{L}{Y_{il}} \gamma_i \ver^{-1/2}\\
&\quad- \fM \ver^{-3/2} \sum_{l}^{M} \dr{L}{Y_{rl}} \gamma_l\\
&\quad- \fM \ver^{-3/2} \sum_{l}^{M} \dr{L}{Y_{rl}} \gamma_l (X_{rl} X_{ri} - X_{ri} \mu_r - X_{rl} \mu_r - \mu_r^2) \\
&= \dr{L}{Y_{il}} \gamma_i \ver^{-1/2}\\
&\quad- \fM \ver^{-3/2} \sum_{l}^{M} \dr{L}{Y_{rl}} \gamma_l (1 - X_{ri} \mu_r + \mu_r^2)\\
&\quad- \fM \ver^{-3/2} \sum_{l}^{M} \dr{L}{Y_{rl}} \gamma_l X_{rl} (X_{ri} - \mu_r) \\
&= \dr{L}{Y_{il}} \gamma_i \ver^{-1/2}\\
&\quad- \fM \ver^{-3/2} (1 - X_{ri} \mu_r + \mu_r^2) \sum_{l}^{M} \dr{L}{Y_{rl}} \gamma_l\\
&\quad- \fM \ver^{-3/2} (X_{ri} - \mu_r) \sum_{l}^{M} \dr{L}{Y_{rl}} \gamma_l X_{rl} \\
&= \dr{L}{Y_{il}} \gamma_i \ver^{-1/2}\\
&\quad- \fM \ver^{-3/2} (1 - X_{ri} \mu_r + \mu_r^2) \left(\dr{L}{\bt{Y}_{r:}}\right) \bt{\gamma}\\
&\quad- \fM \ver^{-3/2} (X_{ri} - \mu_r) \left(\left(\dr{L}{\bt{Y}_{r:}}\right) \circ \bt{X}_{r:}\right) \bt{\gamma} \\
\end{aligned}

After looking long and hard at the shapes of these elements, it is possible to see that the derivative towards $\bt{X}$ can be written as:

\begin{aligned}
\dr{L}{\bt X} =&\dr{L}{\bt Y} \circ (\veb^{-1/2} \boldsymbol{\gamma}^T)\\
&- \fM \veb^{-3/2} \circ (\dr{L}{\bt Y} \boldsymbol{\gamma}) \circ (1 - \bt X \circ \boldsymbol{\mu}^T + \boldsymbol{\mu} \circ \boldsymbol{\mu})\\
&- \fM \veb^{-3/2} \circ \left(( \dr{L}{\bt Y} \circ \bt X ) \boldsymbol{\gamma}\right) \circ (\bt X - \boldsymbol{\mu})
\end{aligned}
#+END_EXPORT

*** b

    See =custom_layergrad.py=.

*** c

    See =custom_layergrad.py=.

*** d

Layer normalization was created in response to batch normalization. Batch normalization
was found to reduce training time and thus proving very useful, but it had some
limitations. First, the efficacy of batch normalization greatly depended on the
batch size, and specifically did not work well for small batch sizes. Second,
batch normalization could not be applied to Recurrent Neural Networks (RNNs),
because the variability of input sequences is not compatible with the
assumptions batch normalization requires.

Layer normalization is a way to perform normalization in between modules while
not being plagued by these two issues. Because layer normalization functions
separately on each layer in a single sample, its performance is independent of the batch
size. In addition, this makes it work for RNNs too, because the length of the
input sequence is irrelevant for its functioning.

So what are limitations of layer normalization? Well, layer normalization
obviously normalizes each sample individually, which means that each sample
will have the same mean and variance. In the case that the mean and variance of
samples carry meaningful information for the task at hand, this information
gets lost after a =LayerNorm= module, so the network has to express this before
the first =LayerNorm= module. This is not the case for BatchNorm, because the
mean and variance is calculated over multiple samples, so the differences in
these aggregates between samples is maintained.

* PyTorch CNN

** a

My implementation of this ConvNet does indeed reach a final accuracy of
=79.930= on the default parameters. The loss and accuracy during training of
this run are displayed in Figure [[fig:convnet_results]]

#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+CAPTION: Loss and accuracy during training of the PyTorch ConvNet on default params. The accuracy is graphed using a dashed line.
#+label: fig:convnet_results
[[file:code/1604922308_convnet_results.png]]
