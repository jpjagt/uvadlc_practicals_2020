#+BIND: org-export-use-babel nil
#+TITLE: DLC assignment 3: generations of generators
#+AUTHOR: jeroen jagt
#+EMAIL: <jpjagt@pm.me>
#+DATE: December 8, 2020
#+LATEX: \setlength\parindent{0pt}
#+LaTeX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LATEX_HEADER_EXTRA:  \usepackage{mdframed}
#+LATEX_HEADER_EXTRA: \BeforeBeginEnvironment{minted}{\begin{mdframed}}
#+LATEX_HEADER_EXTRA: \AfterEndEnvironment{minted}{\end{mdframed}}
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session report :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex

* VAE

** Q1.1

We have two distributions: one over the latent space, and one over the image
space (given a point in the latent space). Therefore, we can sample a fresh
random image by first sampling $\tilde{z}$, and then $\tilde{x}$:
$$\tilde{z} \sim \mathcal{N}(0, \bm{I}_D)$$
$$\tilde{x}^{(m)} \sim \text{Bern}(f_\theta(\tilde{z})_m) \quad \forall_{m \in M}$$

** Q1.2

Well, when sampling from $p(\bm{z}_n)$, we have infinite space to sample
from. However, a relatively small part of that latent space will actually be
likely to have generated $\bm{x}_n$ (i.e. a small part will lead to a
$p(\bm{x}_n | \bm{z}_n)$ larger than some negliglable probability). So, when
sampling $\bm{z}_n$ like that, a lot of the samples you generate will be
irrelevant for approximating $\log p(\bm{x}_n)$. The same goes for every
dimension of the latent space, and thus, a higher dimension will mean even less
relevant samples.

** Q1.3

*small KL-divergence*: $(0, 0, 1, 1)$ (both distributions are the same)

*large KL-divergence*: $(10000, -100000, 459060, 0.00023)$ (wildly different
 distributions that are very divergent)

** Q1.4

It's all due to the difference between the ELBO and $\log p(\bm{x}_n)$, which
is a KL-divergence of our approximation $q(Z|\bm{x}_n)$ and the real
$p(Z|\bm{x}_n)$. This KL-divergence has to be equal to or greater than zero,
and therefore, we know that the ELBO is a lower-bound of $\log p(\bm{x}_n)$.

We have to optimize the lower-bound, because we cannot compute $\log p(\bm{x}_n)$
directly, nor can we compute this difference $KL(q(Z|\bm{x}_n) ||
p(Z|\bm{x}_n))$, because we do not know the real distribution
$p(Z|\bm{x}_n)$. If we would know that, we could move it to the right-hand side
and gain a (computable) expression for $\log p(\bm{x}_n)$, but alas, we do not,
and have to resort to optimizing the lower-bound (which will at least increase
$\log p(\bm{x}_n)$).

** Q1.5

The lower bound can (only) increase either when:
- the first term increases. This essentially means that the likelihood that our
  data was generated by the latent distribution described by $q(Z|\bm{x}_n)$,
  increases.
- the second term decreases (i.e., that that KL-divergence
  decreases). Intuitively, a decrease in this KL-divergence happens if
  $q(Z|\bm{x}_n)$ becomes more similar to our prior $p(Z)$.

So we see that the lower bound increases as our $q(Z|\bm{x}_n)$ better
'explains' our observations (data), and as it grows closer to the prior $p(Z)$.

** Q1.6

- *reconstruction*: the higher the probability that $q_\phi(Z|\bm{x}_n)$
  describes the data $\bm{x}_n$, the lower this term. Ergo, this term denotes
  the accuracy of the reconstruction of $\bm{x}_n$, given a $Z$ which is in
  turn generated by (a function of) the data -- hence, a reconstruction.

- *regularization*: if our trainable distribution $q_\phi(Z|\bm{x}_n)$ diverges
  too much from what we expect the distribution to be like, the loss increases
  due to this term, which thus regularizes the learnable parameters $\phi$.

** Q1.7

#+BEGIN_EXPORT latex
\newcommand{\qqq}{q_\phi(Z|\bm{x}_n)}
\newcommand{\sq}{\text{diag}(\Sigma_\phi (\bm{x}_n))}
\begin{align*}
\mathcal{L}_n^{\text{recon}} &= -\mathbb{E}_{\qqq} \[ \log p_\theta(\bm{x}_n | Z) \]\\
&\approx - \frac{1}{L} \sum^L_{l=1} p_\theta(\bm{x}_n | Z^{(l)}) \quad Z^{(l)} \sim \qqq\\
&= - \frac{1}{L} \sum^L_{l=1} p_\theta(\bm{x}_n | Z^{(l)}) \quad Z^{(l)} \sim \qqq\\
\mathcal{L}_n^{\text{reg}} &= D_{\text{KL}}(\q || p_\theta(Z))\\
&= D_{\text{KL}}(\mathcal{N}(\mu_\phi(\bm{x}_n), \text{diag}(\Sigma_\phi (\bm{x}_n))) || \mathcal{N}(0, \bm{I}_D))\\
&= \sum_{i} D_{\text{KL}}(\mathcal{N}(\mu_\phi(\bm{x}_n)_i, \Sigma_\phi (\bm{x}_n)_i) || \mathcal{N}(0, 1))\\
&= \sum_{i} \frac{1}{2} \left( \Sigma_\phi (\bm{x}_n)_i + \mu_\phi(\bm{x}_n)_i^2 - 1 - \log \Sigma_\phi (\bm{x}_n)_i} \right)
\end{align*}

And so,

\begin{align*}
\mathcal{L} &= \sum_{n=1}^N \mathcal{L}_n^{\text{recon}} + \mathcal{L}_n^{\text{reg}}\\
&\approx \sum_{n=1}^N \left( \sum_{i} \frac{1}{2} \left( \Sigma_\phi (\bm{x}_n)_i + \mu_\phi(\bm{x}_n)_i^2 - 1 - \log \Sigma_\phi (\bm{x}_n)_i} \right) - \frac{1}{L} \sum^L_{l=1} p_\theta(\bm{x}_n | Z^{(l)}) \right) \quad Z^{(l)} \sim \qqq
\end{align*}
#+END_EXPORT

** Q1.8

The operation of sampling is non-differentiable, because it does not (unlike a
function) exist in some spatial domain. Hence, it has no surface, and a
gradient or slope cannot be taken. Sampling is as much differentiable as the
action of eating an apple pie is, say. Because, however, sampling is used in
the function $\mathcal{L}$, the derivative gets 'stuck' at the operation of
sampling and cannot be backpropagated beyond sampling, while we still have
parameters there (in the function that is being sampled from). The
reparametrization trick separates the function that is being sampled from,
$\qqq$, from the sampling itself. This makes the backpropagation towards the
parameters in that function 'before' the sampling independent of the sampling
operation, which makes the gradients of those parameters computable.

** Q1.9

The encoder and decoder are both fully connected feed-forward NNs, both
consisting of a sequence of alternating =Linear= and =ReLU= layers, with the final
=ReLU= layer omitted (i.e. the final layer is a =Linear= one). Both the encoder
and the decoder contain a single hidden =Linear= layer of 512 nodes (making three
=Linear= layers in total).

The outputs of the encoder are used as the means and log-standard-deviations of
the Gaussian distribution over each of the latent dimensions. During a forward
pass, reparametrized sampling is used to find a =z= that represents the image
in the latent dimension, and which gets used as input to the decoder.

The optimizer used for training is =Adam= with a learning rate of =1e-3=. The
batch size used is =128=. The training code was implemented using =PyTorch
Lightning=. Results of the training can be seen in [[fig:bpd_during_training]]
(note that the test bpd represents a single digit, rather than the sequence of
bpd on the test set during training).

#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+CAPTION: The bits per dimension on training and validation set during training of the MLP VAE. The green horizontal line shows the value of the test bpd *on the final iteration only*.
#+label: fig:bpd_during_training
[[file:./part1/plots/bpd_mlp.png]]

** Q1.10

Samples of the MLP VAE after training for 0, 10, and 80 epochs
are displayed in Figures [[fig:vae_samples_at_0]],
[[fig:vae_samples_at_10]], and [[fig:vae_samples_at_80]], respectively. Before
starting training, we observe that the VAE outputs white noise (Figure
[[fig:vae_samples_at_0]]). After 10 epochs, we observe that the VAE outputs images
similar in style to the dataset, but the digits are barely legible and not very
realistic (Figure [[fig:vae_samples_at_10]]). After 80 epochs, the quality of the
generated digits has improved further: most digits are recognizable, even
though there are some that are too thin or in-between two different digits
(Figure [[fig:vae_samples_at_80]]). However, keep in mind that we are performing
normal sampling in
the latent space, and that some areas in the latent space are not
representative of real-world data (e.g., areas that transition from one
realistic digit to another).

#+ATTR_LATEX: :width 0.6\linewidth :float nil
#+CAPTION: Samples of the MLP VAE after training for 0 epochs.
#+label: fig:vae_samples_at_0
[[file:]][[~/code/uva/msc/dlc/uvadlc_practicals_2020/assignment_3/3_generative/part1/VAE_logs/lightning_logs/version_26/0_samples.png]]

#+ATTR_LATEX: :width 0.6\linewidth :float nil
#+CAPTION: Samples of the MLP VAE after training for 10 epochs.
#+label: fig:vae_samples_at_10
[[file:]][[~/code/uva/msc/dlc/uvadlc_practicals_2020/assignment_3/3_generative/part1/VAE_logs/lightning_logs/version_26/10_samples.png]]

#+ATTR_LATEX: :width 0.6\linewidth :float nil
#+CAPTION: Samples of the MLP VAE after training for 80 epochs.
#+label: fig:vae_samples_at_80
[[file:]][[~/code/uva/msc/dlc/uvadlc_practicals_2020/assignment_3/3_generative/part1/VAE_logs/lightning_logs/version_26/80_samples.png]]

** Q1.11

The manifold visualization is displayed in Figure
[[fig:manifold_visualization]]. Indeed, all digits 0-9 are seen to be represented
in some form throughout the latent space. Interesting to note is that images
that are visually similar (such as the four and nine, which both have a
vertical stem with a circular-ish feature on top) do occur close to each other
in the latent space, which is to be expected due to its continuous nature. We
see that the horizontal axis roughly corresponds to the degree of roundedness
of the digit, while the vertical axis roughly corresponds to the degree of
vertical symmetry.

#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+CAPTION: Visualization of the two-dimensional manifold of VAE trained on MNIST dataset.
#+label: fig:manifold_visualization
[[file:]][[~/code/uva/msc/dlc/uvadlc_practicals_2020/assignment_3/3_generative/part1/VAE_logs/lightning_logs/version_7093801/80_samples.png]]


* GANs

** Q2.1
*a)* The first term consists of an expected value over the log of the output of
the discriminator when the input is some sample from the real dataset, which
ensures that the discriminator outputs a positive label for real samples. The
second term consists of the expected value of the log of one minus the output
of the discriminator when it is passed a generated sample, which ensures that
the discriminator outputs a negative label (small value) for generated
samples. The discriminator will optimize for this because it is trying to
maximize the objective, while the generator will optimize for the reverse
situation (positive labels for generated samples and negative labels for real
samples), because it is trying to minimize the objective.

*b)* If the model has converged, we have that, one: $p_{data}(x) = p_{gen}(x)$,
and (leading from that), two: $D(x) = \frac{1}{2}$ (because it is not able to
distinguish between real and generated data). If we insert this into the
objective, we get

#+BEGIN_EXPORT latex
\begin{align*}
V(D^{*}, G^{*}) &= \mathbb{E}_{p_{data}(x)} \log \frac{1}{2} + \mathbb{E}_{p_{z}(z)} \log (1 - \frac{1}{2})\\
&= \log \frac{1}{2} + \log (1 - \frac{1}{2})\\
&= -\log 2 - \log 2\\
&= -2\log 2
\end{align*}
#+END_EXPORT

** Q2.2

Early on during training, both $D$ and $G$ are terrible at their job. However,
since $D$ can learn its task more quickly than $G$, we soon get that $\log(1 -
D(G(Z))) \approx \log(1 - 0) = 0$. This means that the gradients towards $\theta_G$,
which incorporate this small value into their product, will be very small as
well.
The solution to this problem is to define a separate objective for the
generator $G$, which incorporates the term $\log (D(G(Z)))$. This value will
(only) increase as $D(G(Z)) \rightarrow 1$, and will otherwise be large enough
to prevent vanishing gradients.

** Q2.3


** Q2.4


** Q2.5
